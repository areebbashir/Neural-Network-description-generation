{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import re\n",
        "import io\n",
        "import sys\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "import pathlib\n",
        "import textwrap\n",
        "import time\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "Y0X_9ohcBVd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DynamicConvNet(nn.Module):\n",
        "    def __init__(self, in_channels, num_layers, layer_channels, kernel_sizes, paddings, pool_size, dropout_probs, use_batch_norm, use_avg_pool, num_classes):\n",
        "        super(DynamicConvNet, self).__init__()\n",
        "        layers = []\n",
        "\n",
        "        # Add convolutional layers\n",
        "        for i in range(num_layers):\n",
        "            layers.append(\n",
        "                nn.Conv2d(\n",
        "                    in_channels=in_channels if i == 0 else layer_channels[i-1],\n",
        "                    out_channels=layer_channels[i],\n",
        "                    kernel_size=kernel_sizes[i],\n",
        "                    stride=1,\n",
        "                    padding=paddings[i]\n",
        "                )\n",
        "            )\n",
        "            if use_batch_norm[i]:\n",
        "                layers.append(nn.BatchNorm2d(layer_channels[i]))\n",
        "            layers.append(nn.ReLU(inplace=True))\n",
        "            if dropout_probs[i] > 0:\n",
        "                layers.append(nn.Dropout(dropout_probs[i]))\n",
        "            if (i + 1) % 2 == 0:  # Add pooling after every second conv layer\n",
        "                if use_avg_pool[i]:\n",
        "                    layers.append(nn.AvgPool2d(kernel_size=pool_size, stride=pool_size))\n",
        "                else:\n",
        "                    layers.append(nn.MaxPool2d(kernel_size=pool_size, stride=pool_size))\n",
        "\n",
        "        self.conv_layers = nn.Sequential(*layers)\n",
        "\n",
        "        # Calculate the size of the flattened feature map after convolutions and pooling\n",
        "        self.fc_input_size = self._get_fc_input_size(in_channels, num_layers, layer_channels, kernel_sizes, paddings, pool_size)\n",
        "\n",
        "        self.fc = nn.Linear(self.fc_input_size, num_classes)\n",
        "\n",
        "    def _get_fc_input_size(self, in_channels, num_layers, layer_channels, kernel_sizes, paddings, pool_size):\n",
        "        # Assuming input size of (3, 32, 32)\n",
        "        input_size = 32\n",
        "        for i in range(num_layers):\n",
        "            if paddings[i] == 'same':\n",
        "                output_size = input_size\n",
        "            else:\n",
        "                output_size = (input_size - kernel_sizes[i] + 2 * paddings[i]) + 1\n",
        "            if (i + 1) % 2 == 0:  # Apply pooling\n",
        "                output_size = output_size // pool_size\n",
        "            input_size = output_size\n",
        "\n",
        "        return layer_channels[-1] * output_size * output_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.view(-1, self.fc_input_size)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "def create_models(num_models):\n",
        "    models = []\n",
        "    for i in range(num_models):\n",
        "        num_layers = (i % 30) + 1  # Vary the number of layers between 1 and 30\n",
        "        layer_channels = [6 * (j + 1) for j in range(num_layers)]  # Increment channels by 6 for each layer\n",
        "        kernel_sizes = [[3, 5, 7, 9, 11][i//60] for _ in range(num_layers)]  # Using kernel size between [3, 5, 7, 9, 11] for simplicity\n",
        "        paddings = [1 for _ in range(num_layers)]  # Using padding of 1 for simplicity\n",
        "        pool_size = 2  # Pooling size\n",
        "        dropout_probs = [0.5 if j % 2 == 0 else 0 for j in range(num_layers)]  # Dropout every second layer\n",
        "        use_batch_norm = [True if j % 2 == 1 else False for j in range(num_layers)]  # Batch norm every second layer\n",
        "        use_avg_pool = [True if j % 2 == 0 else False for j in range(num_layers)]  # Average pooling every second layer\n",
        "        num_classes = 10\n",
        "\n",
        "        model = DynamicConvNet(3, num_layers, layer_channels, kernel_sizes, paddings, pool_size, dropout_probs, use_batch_norm, use_avg_pool, num_classes)\n",
        "        models.append(model)\n",
        "\n",
        "    return models\n",
        "\n",
        "cnn_models = create_models(240)\n"
      ],
      "metadata": {
        "id": "y57R_rOX85Rk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "909f7956-e153-477a-ff71-945217b4ca25"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/init.py:452: UserWarning: Initializing zero-element tensors is a no-op\n",
            "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DynamicLSTMNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_sizes, num_layers, dropout_probs, bidirectional, num_classes):\n",
        "        super(DynamicLSTMNet, self).__init__()\n",
        "        self.lstm_layers = nn.ModuleList()\n",
        "        self.dropouts = nn.ModuleList()\n",
        "\n",
        "        for i in range(num_layers):\n",
        "            input_dim = input_size if i == 0 else hidden_sizes[i-1]\n",
        "            self.lstm_layers.append(\n",
        "                nn.LSTM(\n",
        "                    input_size=input_dim,\n",
        "                    hidden_size=hidden_sizes[i],\n",
        "                    num_layers=1,\n",
        "                    batch_first=True,\n",
        "                    bidirectional=bidirectional[i]\n",
        "                )\n",
        "            )\n",
        "            self.dropouts.append(nn.Dropout(dropout_probs[i]))\n",
        "\n",
        "        self.fc_input_size = hidden_sizes[-1] * (2 if bidirectional[-1] else 1)\n",
        "        self.fc = nn.Linear(self.fc_input_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for lstm, dropout in zip(self.lstm_layers, self.dropouts):\n",
        "            x, _ = lstm(x)\n",
        "            x = dropout(x)\n",
        "\n",
        "        x = x[:, -1, :]  # Get the last output of the sequence\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "def create_lstm_models(num_models):\n",
        "    models = []\n",
        "    for i in range(num_models):\n",
        "        input_size = 10  # Example input size (e.g., number of features in time series data)\n",
        "        num_layers = (i % 20) + 1  # Vary the number of layers between 1 and 20\n",
        "        hidden_sizes = [[4, 8, 16, 32, 64, 128][i//50] * (j + 1) for j in range(num_layers)]  # Increment hidden size by [4, 8, 16, 32, 64, 128] for each layer\n",
        "        dropout_probs = [0.5 if j % 2 == 0 else 0 for j in range(num_layers)]  # Dropout every second layer\n",
        "        bidirectional = [True if j % 2 == 1 else False for j in range(num_layers)]  # Bidirectional every second layer\n",
        "        num_classes = 10\n",
        "\n",
        "        model = DynamicLSTMNet(input_size, hidden_sizes, num_layers, dropout_probs, bidirectional, num_classes)\n",
        "        models.append(model)\n",
        "\n",
        "    return models\n",
        "\n",
        "lstm_models = create_lstm_models(200)\n",
        "\n"
      ],
      "metadata": {
        "id": "BqhG68dPDSG4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DynamicTransformer(nn.Module):\n",
        "    def __init__(self, input_dim, model_dim, num_heads, num_encoder_layers, num_decoder_layers, ff_dim, dropout_probs, num_classes):\n",
        "        super(DynamicTransformer, self).__init__()\n",
        "\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=model_dim, nhead=num_heads, dim_feedforward=ff_dim, dropout=dropout_probs[0])\n",
        "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_encoder_layers)\n",
        "\n",
        "        self.decoder_layer = nn.TransformerDecoderLayer(d_model=model_dim, nhead=num_heads, dim_feedforward=ff_dim, dropout=dropout_probs[1])\n",
        "        self.decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=num_decoder_layers)\n",
        "\n",
        "        self.fc_in = nn.Linear(input_dim, model_dim)\n",
        "        self.fc_out = nn.Linear(model_dim, num_classes)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_probs[2])\n",
        "        self.batch_norm = nn.BatchNorm1d(model_dim)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src = self.fc_in(src)\n",
        "        tgt = self.fc_in(tgt)\n",
        "\n",
        "        memory = self.encoder(src)\n",
        "        output = self.decoder(tgt, memory)\n",
        "\n",
        "        output = self.batch_norm(output.permute(1, 2, 0)).permute(2, 0, 1)\n",
        "        output = self.dropout(output)\n",
        "\n",
        "        output = self.fc_out(output)\n",
        "        return output\n",
        "\n",
        "def create_transformer_models(num_models):\n",
        "    models = []\n",
        "    for i in range(num_models):\n",
        "        input_dim = [32, 64, 128, 256, 512][i//50]  # Input dimension between\n",
        "        model_dim = [16, 32, 64, 128, 256][i//50]  # Model dimension between [16, 32, 64, 128, 256]\n",
        "        num_heads = 8  # Number of heads in multi-head attention\n",
        "        num_encoder_layers = (i % 8) + 1  # Vary the number of encoder layers between 1 and 6\n",
        "        num_decoder_layers = (i % 8) + 1  # Vary the number of decoder layers between 1 and 6\n",
        "        ff_dim = 512  # Feedforward dimension\n",
        "        dropout_probs = [0.1, 0.1, 0.1]  # Dropout probabilities for encoder, decoder, and final layers\n",
        "        num_classes = 10\n",
        "\n",
        "        model = DynamicTransformer(input_dim, model_dim, num_heads, num_encoder_layers, num_decoder_layers, ff_dim, dropout_probs, num_classes)\n",
        "        models.append(model)\n",
        "\n",
        "    return models\n",
        "\n",
        "transformer_models = create_transformer_models(200)\n",
        "\n"
      ],
      "metadata": {
        "id": "LiO7pAZKEWKr",
        "outputId": "fe2c37bb-a662-459f-f846-b660b4736084",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DynamicGRUNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_sizes, num_layers, dropout_probs, bidirectional, num_classes):\n",
        "        super(DynamicGRUNet, self).__init__()\n",
        "        self.gru_layers = nn.ModuleList()\n",
        "        self.dropouts = nn.ModuleList()\n",
        "\n",
        "        for i in range(num_layers):\n",
        "            input_dim = input_size if i == 0 else hidden_sizes[i-1]\n",
        "            self.gru_layers.append(\n",
        "                nn.GRU(\n",
        "                    input_size=input_dim,\n",
        "                    hidden_size=hidden_sizes[i],\n",
        "                    num_layers=1,\n",
        "                    batch_first=True,\n",
        "                    bidirectional=bidirectional[i]\n",
        "                )\n",
        "            )\n",
        "            self.dropouts.append(nn.Dropout(dropout_probs[i]))\n",
        "\n",
        "        self.fc_input_size = hidden_sizes[-1] * (2 if bidirectional[-1] else 1)\n",
        "        self.fc = nn.Linear(self.fc_input_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for gru, dropout in zip(self.gru_layers, self.dropouts):\n",
        "            x, _ = gru(x)\n",
        "            x = dropout(x)\n",
        "\n",
        "        x = x[:, -1, :]  # Get the last output of the sequence\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "def create_gru_models(num_models):\n",
        "    models = []\n",
        "    for i in range(num_models):\n",
        "        input_size = 10  # Example input size (e.g., number of features in time series data)\n",
        "        num_layers = (i % 20) + 1  # Vary the number of layers between 1 and 20\n",
        "        hidden_sizes = [[4, 8, 16, 32, 64][i//50] * (j + 1) for j in range(num_layers)]  # Increment hidden size by [4, 8, 16, 32, 64] for each layer\n",
        "        dropout_probs = [[0.1, 0.2,0.3, 0.4, 0.5][i//50] if j % 2 == 0 else 0 for j in range(num_layers)]  # Dropout every second layer\n",
        "        bidirectional = [True if j % 2 == 1 else False for j in range(num_layers)]  # Bidirectional every second layer\n",
        "        num_classes = 10\n",
        "\n",
        "        model = DynamicGRUNet(input_size, hidden_sizes, num_layers, dropout_probs, bidirectional, num_classes)\n",
        "        models.append(model)\n",
        "\n",
        "    return models\n",
        "\n",
        "gru_models = create_gru_models(200)\n"
      ],
      "metadata": {
        "id": "SrIwu0mLF2ad"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DynamicRNNNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_sizes, num_layers, dropout_probs, bidirectional, num_classes):\n",
        "        super(DynamicRNNNet, self).__init__()\n",
        "        self.rnn_layers = nn.ModuleList()\n",
        "        self.dropouts = nn.ModuleList()\n",
        "\n",
        "        for i in range(num_layers):\n",
        "            input_dim = input_size if i == 0 else hidden_sizes[i-1]\n",
        "            self.rnn_layers.append(\n",
        "                nn.RNN(\n",
        "                    input_size=input_dim,\n",
        "                    hidden_size=hidden_sizes[i],\n",
        "                    num_layers=1,\n",
        "                    batch_first=True,\n",
        "                    bidirectional=bidirectional[i]\n",
        "                )\n",
        "            )\n",
        "            self.dropouts.append(nn.Dropout(dropout_probs[i]))\n",
        "\n",
        "        self.fc_input_size = hidden_sizes[-1] * (2 if bidirectional[-1] else 1)\n",
        "        self.fc = nn.Linear(self.fc_input_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for rnn, dropout in zip(self.rnn_layers, self.dropouts):\n",
        "            x, _ = rnn(x)\n",
        "            x = dropout(x)\n",
        "\n",
        "        x = x[:, -1, :]  # Get the last output of the sequence\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "def create_rnn_models(num_models):\n",
        "    models = []\n",
        "    for i in range(num_models):\n",
        "        input_size = 10  # Example input size (e.g., number of features in time series data)\n",
        "        num_layers = (i % 20) + 1  # Vary the number of layers between 1 and 20\n",
        "        hidden_sizes = [[4, 8, 16, 32, 64][i//50] * (j + 1) for j in range(num_layers)]  # Increment hidden size by [4, 8, 16, 32, 64] for each layer\n",
        "        dropout_probs = [[0.1, 0.2,0.3, 0.4, 0.5][i//50] if j % 2 == 0 else 0 for j in range(num_layers)]  # Dropout every second layer\n",
        "        bidirectional = [True if j % 2 == 1 else False for j in range(num_layers)]  # Bidirectional every second layer\n",
        "        num_classes = 10\n",
        "\n",
        "        model = DynamicRNNNet(input_size, hidden_sizes, num_layers, dropout_probs, bidirectional, num_classes)\n",
        "        models.append(model)\n",
        "\n",
        "    return models\n",
        "\n",
        "# Create 50 models\n",
        "rnn_models = create_rnn_models(200)\n"
      ],
      "metadata": {
        "id": "TqzBFmIJDHdq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_models = [cnn_models, lstm_models, transformer_models, rnn_models, gru_models]"
      ],
      "metadata": {
        "id": "Epguly17GS2Q"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_dynamic_convnet(arch_string):\n",
        "    conv_regex = r'Conv2d\\((\\d+), (\\d+), kernel_size=\\((\\d+), (\\d+)\\), stride=\\((\\d+), (\\d+)\\), padding=\\((\\d+), (\\d+)\\)\\)'\n",
        "    relu_regex = r'ReLU\\(inplace=(True|False)\\)'\n",
        "    dropout_regex = r'Dropout\\(p=(0\\.\\d+), inplace=(False|True)\\)'\n",
        "    batch_norm_regex = r'BatchNorm2d\\((\\d+), eps=(\\d+e-\\d+), momentum=(\\d+e-\\d+), affine=(True|False), track_running_stats=(True|False)\\)'\n",
        "    maxpool_regex = r'MaxPool2d\\(kernel_size=(\\d+), stride=(\\d+), padding=(\\d+), dilation=(\\d+), ceil_mode=(False|True)\\)'\n",
        "    linear_regex = r'Linear\\(in_features=(\\d+), out_features=(\\d+), bias=(True|False)\\)'\n",
        "\n",
        "    conv_layers = re.findall(conv_regex, arch_string)\n",
        "    relu_layers = re.findall(relu_regex, arch_string)\n",
        "    dropout_layers = re.findall(dropout_regex, arch_string)\n",
        "    batch_norm_layers = re.findall(batch_norm_regex, arch_string)\n",
        "    maxpool_layers = re.findall(maxpool_regex, arch_string)\n",
        "    linear_layer = re.findall(linear_regex, arch_string)\n",
        "\n",
        "    structured_string = \"\"\n",
        "\n",
        "    # Add Conv layers\n",
        "    layer_index = 1\n",
        "    for (in_channels, out_channels, kernel_height, kernel_width, stride_height, stride_width, padding_height, padding_width) in conv_layers:\n",
        "        structured_string += f\"Layer{layer_index}(Type=Conv2d | In_channels={in_channels} | Out_channels={out_channels} | Kernel_size=({kernel_height},{kernel_width}) | Stride=({stride_height},{stride_width}) | Padding=({padding_height},{padding_width}))\\n\"\n",
        "        layer_index += 1\n",
        "\n",
        "    # Add ReLU layers\n",
        "    for inplace in relu_layers:\n",
        "        structured_string += f\"Layer{layer_index}(Type=ReLU | Inplace={inplace})\\n\"\n",
        "        layer_index += 1\n",
        "\n",
        "    # Add Dropout layers\n",
        "    for p, inplace in dropout_layers:\n",
        "        structured_string += f\"Layer{layer_index}(Type=Dropout | Probability={p} | Inplace={inplace})\\n\"\n",
        "        layer_index += 1\n",
        "\n",
        "    # Add BatchNorm layers\n",
        "    for (num_features, eps, momentum, affine, track_running_stats) in batch_norm_layers:\n",
        "        structured_string += f\"Layer{layer_index}(Type=BatchNorm2d | Num_features={num_features} | Eps={eps} | Momentum={momentum} | Affine={affine} | Track_running_stats={track_running_stats})\\n\"\n",
        "        layer_index += 1\n",
        "\n",
        "    # Add MaxPool layers\n",
        "    for (kernel_size, stride, padding, dilation, ceil_mode) in maxpool_layers:\n",
        "        structured_string += f\"Layer{layer_index}(Type=MaxPool2d | Kernel_size={kernel_size} | Stride={stride} | Padding={padding} | Dilation={dilation} | Ceil_mode={ceil_mode})\\n\"\n",
        "        layer_index += 1\n",
        "\n",
        "    # Add Linear layer\n",
        "    if linear_layer:\n",
        "        in_features, out_features, bias = linear_layer[0]\n",
        "        structured_string += f\"Layer{layer_index}(Type=Linear | In_features={in_features} | Out_features={out_features} | Bias={bias})\"\n",
        "\n",
        "    return structured_string\n",
        "\n",
        "def parse_dynamic_recurrent_net(arch_string, rnn_type):\n",
        "    rnn_regex = fr'{rnn_type}\\((\\d+), (\\d+), batch_first=True(, bidirectional=True)?\\)'\n",
        "    dropout_regex = r'Dropout\\(p=(0\\.\\d+), inplace=(False|True)\\)'\n",
        "    linear_regex = r'Linear\\(in_features=(\\d+), out_features=(\\d+), bias=(True|False)\\)'\n",
        "\n",
        "    rnn_layers = re.findall(rnn_regex, arch_string)\n",
        "    dropout_layers = re.findall(dropout_regex, arch_string)\n",
        "    linear_layer = re.findall(linear_regex, arch_string)\n",
        "\n",
        "    structured_string = \"\"\n",
        "\n",
        "    # Add RNN layers\n",
        "    layer_index = 1\n",
        "    for (input_size, hidden_size, bidirectional) in rnn_layers:\n",
        "        bidirectional = True if bidirectional else False\n",
        "        structured_string += f\"Layer{layer_index}(Type={rnn_type} | Input_size={input_size} | Hidden_size={hidden_size} | Bidirectional={bidirectional})\\n\"\n",
        "        layer_index += 1\n",
        "\n",
        "    # Add Dropout layers\n",
        "    for p, inplace in dropout_layers:\n",
        "        structured_string += f\"Layer{layer_index}(Type=Dropout | Probability={p} | Inplace={inplace})\\n\"\n",
        "        layer_index += 1\n",
        "\n",
        "    # Add Linear layer\n",
        "    if linear_layer:\n",
        "        in_features, out_features, bias = linear_layer[0]\n",
        "        structured_string += f\"Layer{layer_index}(Type=Linear | In_features={in_features} | Out_features={out_features} | Bias={bias})\"\n",
        "\n",
        "    return structured_string\n",
        "\n",
        "def parse_dynamic_transformer(arch_string):\n",
        "    mha_regex = r'MultiheadAttention\\(\\s*\\(out_proj\\): NonDynamicallyQuantizableLinear\\(in_features=(\\d+), out_features=(\\d+), bias=(True|False)\\)\\s*\\)'\n",
        "    linear_regex = r'Linear\\(in_features=(\\d+), out_features=(\\d+), bias=(True|False)\\)'\n",
        "    dropout_regex = r'Dropout\\(p=(0\\.\\d+), inplace=(False|True)\\)'\n",
        "    layer_norm_regex = r'LayerNorm\\(\\((\\d+),\\), eps=(\\d+e-\\d+), elementwise_affine=(True|False)\\)'\n",
        "    batch_norm_regex = r'BatchNorm1d\\((\\d+), eps=(\\d+e-\\d+), momentum=(\\d+e-\\d+), affine=(True|False), track_running_stats=(True|False)\\)'\n",
        "\n",
        "    mha_layers = re.findall(mha_regex, arch_string)\n",
        "    linear_layers = re.findall(linear_regex, arch_string)\n",
        "    dropout_layers = re.findall(dropout_regex, arch_string)\n",
        "    layer_norm_layers = re.findall(layer_norm_regex, arch_string)\n",
        "    batch_norm_layer = re.findall(batch_norm_regex, arch_string)\n",
        "\n",
        "    structured_string = \"\"\n",
        "    layer_index = 1\n",
        "\n",
        "    # Add MultiheadAttention layers\n",
        "    for (in_features, out_features, bias) in mha_layers:\n",
        "        structured_string += f\"Layer{layer_index}(Type=MultiheadAttention | In_features={in_features} | Out_features={out_features} | Bias={bias})\\n\"\n",
        "        layer_index += 1\n",
        "\n",
        "    # Add Linear layers\n",
        "    for (in_features, out_features, bias) in linear_layers:\n",
        "        structured_string += f\"Layer{layer_index}(Type=Linear | In_features={in_features} | Out_features={out_features} | Bias={bias})\\n\"\n",
        "        layer_index += 1\n",
        "\n",
        "    # Add Dropout layers\n",
        "    for p, inplace in dropout_layers:\n",
        "        structured_string += f\"Layer{layer_index}(Type=Dropout | Probability={p} | Inplace={inplace})\\n\"\n",
        "        layer_index += 1\n",
        "\n",
        "    # Add LayerNorm layers\n",
        "    for (normalized_shape, eps, elementwise_affine) in layer_norm_layers:\n",
        "        structured_string += f\"Layer{layer_index}(Type=LayerNorm | Normalized_shape={normalized_shape} | Eps={eps} | Elementwise_affine={elementwise_affine})\\n\"\n",
        "        layer_index += 1\n",
        "\n",
        "    # Add BatchNorm layer\n",
        "    if batch_norm_layer:\n",
        "        num_features, eps, momentum, affine, track_running_stats = batch_norm_layer[0]\n",
        "        structured_string += f\"Layer{layer_index}(Type=BatchNorm1d | Num_features={num_features} | Eps={eps} | Momentum={momentum} | Affine={affine} | Track_running_stats={track_running_stats})\\n\"\n",
        "\n",
        "    return structured_string\n"
      ],
      "metadata": {
        "id": "SDVziVCJGcog"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def count_parameters(model):\n",
        "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
        "    total_params = 0\n",
        "    for name, parameter in model.named_parameters():\n",
        "        if not parameter.requires_grad:\n",
        "            continue\n",
        "        params = parameter.numel()\n",
        "        total_params += params\n",
        "    return total_params\n",
        "\n",
        "model_descriptions = []\n",
        "model_names = []\n",
        "model_parameters = []\n",
        "model_descriptions_structured = []\n",
        "i = -1\n",
        "for models in all_models:\n",
        "  i+=1\n",
        "  for j in range(len(models)):\n",
        "    captured_output = io.StringIO()\n",
        "    # Redirect standard output temporarily to the buffer\n",
        "    sys.stdout = captured_output\n",
        "    print(models[j])\n",
        "    # Restore standard output\n",
        "    sys.stdout = sys.__stdout__\n",
        "\n",
        "    # Access the captured output as a string\n",
        "    model_arch_string = captured_output.getvalue()\n",
        "    model_descriptions.append(model_arch_string)\n",
        "    model_names.append(models[j].__class__.__name__)\n",
        "    model_parameters.append(count_parameters(models[j]))\n",
        "    if i==0:\n",
        "      model_descriptions_structured.append(parse_dynamic_convnet(model_arch_string))\n",
        "    elif i==1:\n",
        "      model_descriptions_structured.append(parse_dynamic_recurrent_net(model_arch_string, \"LSTM\"))\n",
        "    elif i==4:\n",
        "      model_descriptions_structured.append(parse_dynamic_recurrent_net(model_arch_string, \"GRU\"))\n",
        "    elif i==3:\n",
        "      model_descriptions_structured.append(parse_dynamic_recurrent_net(model_arch_string, \"RNN\"))\n",
        "    elif i==2:\n",
        "      model_descriptions_structured.append(parse_dynamic_transformer(model_arch_string))\n",
        ""
      ],
      "metadata": {
        "id": "CFdcQV-dPbsm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install --upgrade --user google-cloud-aiplatform"
      ],
      "metadata": {
        "id": "wX9oUgzbSV3E",
        "outputId": "4bc5cb36-0e50-4371-e8d7-ca87d3f6edd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 885
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-cloud-aiplatform in /usr/local/lib/python3.10/dist-packages (1.57.0)\n",
            "Collecting google-cloud-aiplatform\n",
            "  Downloading google_cloud_aiplatform-1.59.0-py2.py3-none-any.whl (5.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.16.2)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.27.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (1.24.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (3.20.3)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (24.1)\n",
            "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.8.0)\n",
            "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (3.21.0)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (1.12.3)\n",
            "Requirement already satisfied: shapely<3.0.0dev in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.0.4)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.8.0)\n",
            "Requirement already satisfied: docstring-parser<1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (0.16)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.63.2)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2.31.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.64.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.48.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (4.9)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.3.3)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.7.1)\n",
            "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.13.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->google-cloud-aiplatform) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->google-cloud-aiplatform) (2.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->google-cloud-aiplatform) (4.12.2)\n",
            "Requirement already satisfied: numpy<3,>=1.14 in /usr/local/lib/python3.10/dist-packages (from shapely<3.0.0dev->google-cloud-aiplatform) (1.25.2)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.5.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2024.6.2)\n",
            "Installing collected packages: google-cloud-aiplatform\n",
            "\u001b[33m  WARNING: The script tb-gcp-uploader is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0mSuccessfully installed google-cloud-aiplatform-1.59.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "c6dca3d7f021480c8c9a72ef878479f0"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "genai.configure(api_key=\"AIzaSyCXVsBNtlYe2U9XFcWPStYcf7xaIH7xtxw\")"
      ],
      "metadata": {
        "id": "wEJxtONVUjrt"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gemini_model = genai.GenerativeModel('gemini-1.0-pro')\n"
      ],
      "metadata": {
        "id": "a02kOvBWVWr0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = gemini_model.generate_content(prompt,\n",
        "                                         generation_config=genai.types.GenerationConfig(\n",
        "                                        # Only one candidate for now.\n",
        "                                        candidate_count=1,\n",
        "                                        max_output_tokens=20,\n",
        "                                        temperature=1.0))"
      ],
      "metadata": {
        "id": "qZCuHWMcWddP"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "CnbPlp_klgmT",
        "outputId": "cb4a8ac7-2c58-44fd-afca-a9d9f9512b58"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The model is a DynamicGRUNet, which consists of 20 GRU layers and '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_summary=[]"
      ],
      "metadata": {
        "id": "pSNc4Kevv7mN"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(model_summary), len(model_descriptions)):\n",
        "  prompt = model_descriptions[i] + f\"Total Trainable Params: {model_parameters[i]}\"+\"Give a precise summary of this model like layers, parameters and input and output sizes in a paragraph.\"\n",
        "  try:\n",
        "    response = gemini_model.generate_content(prompt)\n",
        "  except:\n",
        "    print(\"Trying again\")\n",
        "    time.sleep(5)\n",
        "    response = gemini_model.generate_content(prompt)\n",
        "  model_summary.append(response.text)"
      ],
      "metadata": {
        "id": "GW2OJ0-2WiWT"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(model_descriptions)"
      ],
      "metadata": {
        "id": "XO5OtNrLcJz4",
        "outputId": "b2fd8ba9-764c-440f-b5ff-0384bcf8139d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1040"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_info = {\"model_names\": model_names, \"model_descriptions\" : model_descriptions, \"model_descriptions_structured\" : model_descriptions_structured,  \"model_parameters\": model_parameters, \"model_summary\": model_summary}"
      ],
      "metadata": {
        "id": "xJkzf-uLXc8o"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(model_info)"
      ],
      "metadata": {
        "id": "uVd3QEJ1Lp1E"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "unskZxN9M5jP",
        "outputId": "d31aec9c-90a9-4497-f341-79036b04cf9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        }
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      model_names                                 model_descriptions  \\\n",
              "0  DynamicConvNet  DynamicConvNet(\\n  (conv_layers): Sequential(\\...   \n",
              "1  DynamicConvNet  DynamicConvNet(\\n  (conv_layers): Sequential(\\...   \n",
              "2  DynamicConvNet  DynamicConvNet(\\n  (conv_layers): Sequential(\\...   \n",
              "3  DynamicConvNet  DynamicConvNet(\\n  (conv_layers): Sequential(\\...   \n",
              "4  DynamicConvNet  DynamicConvNet(\\n  (conv_layers): Sequential(\\...   \n",
              "\n",
              "                       model_descriptions_structured  model_parameters  \\\n",
              "0  Layer1(Type=Conv2d | In_channels=3 | Out_chann...             61618   \n",
              "1  Layer1(Type=Conv2d | In_channels=3 | Out_chann...             31582   \n",
              "2  Layer1(Type=Conv2d | In_channels=3 | Out_chann...             48904   \n",
              "3  Layer1(Type=Conv2d | In_channels=3 | Out_chann...             22144   \n",
              "4  Layer1(Type=Conv2d | In_channels=3 | Out_chann...             32494   \n",
              "\n",
              "                                       model_summary  \n",
              "0  This model is a Dynamic Convolutional Neural N...  \n",
              "1  This model is a DynamicConvNet consisting of a...  \n",
              "2  This Dynamic Convolutional Neural Network (Dyn...  \n",
              "3  The DynamicConvNet is a Convolutional Neural N...  \n",
              "4  The model is a Dynamic Convolutional Neural Ne...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c6943f71-6765-4f47-87d5-9f385f662105\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model_names</th>\n",
              "      <th>model_descriptions</th>\n",
              "      <th>model_descriptions_structured</th>\n",
              "      <th>model_parameters</th>\n",
              "      <th>model_summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>DynamicConvNet</td>\n",
              "      <td>DynamicConvNet(\\n  (conv_layers): Sequential(\\...</td>\n",
              "      <td>Layer1(Type=Conv2d | In_channels=3 | Out_chann...</td>\n",
              "      <td>61618</td>\n",
              "      <td>This model is a Dynamic Convolutional Neural N...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>DynamicConvNet</td>\n",
              "      <td>DynamicConvNet(\\n  (conv_layers): Sequential(\\...</td>\n",
              "      <td>Layer1(Type=Conv2d | In_channels=3 | Out_chann...</td>\n",
              "      <td>31582</td>\n",
              "      <td>This model is a DynamicConvNet consisting of a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>DynamicConvNet</td>\n",
              "      <td>DynamicConvNet(\\n  (conv_layers): Sequential(\\...</td>\n",
              "      <td>Layer1(Type=Conv2d | In_channels=3 | Out_chann...</td>\n",
              "      <td>48904</td>\n",
              "      <td>This Dynamic Convolutional Neural Network (Dyn...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>DynamicConvNet</td>\n",
              "      <td>DynamicConvNet(\\n  (conv_layers): Sequential(\\...</td>\n",
              "      <td>Layer1(Type=Conv2d | In_channels=3 | Out_chann...</td>\n",
              "      <td>22144</td>\n",
              "      <td>The DynamicConvNet is a Convolutional Neural N...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>DynamicConvNet</td>\n",
              "      <td>DynamicConvNet(\\n  (conv_layers): Sequential(\\...</td>\n",
              "      <td>Layer1(Type=Conv2d | In_channels=3 | Out_chann...</td>\n",
              "      <td>32494</td>\n",
              "      <td>The model is a Dynamic Convolutional Neural Ne...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c6943f71-6765-4f47-87d5-9f385f662105')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c6943f71-6765-4f47-87d5-9f385f662105 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c6943f71-6765-4f47-87d5-9f385f662105');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-cd781053-81ad-4ad1-aae4-d6791340c9f8\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cd781053-81ad-4ad1-aae4-d6791340c9f8')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-cd781053-81ad-4ad1-aae4-d6791340c9f8 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 1040,\n  \"fields\": [\n    {\n      \"column\": \"model_names\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"DynamicLSTMNet\",\n          \"DynamicGRUNet\",\n          \"DynamicTransformer\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model_descriptions\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 392,\n        \"samples\": [\n          \"DynamicConvNet(\\n  (conv_layers): Sequential(\\n    (0): Conv2d(3, 6, kernel_size=(7, 7), stride=(1, 1), padding=(1, 1))\\n    (1): ReLU(inplace=True)\\n    (2): Dropout(p=0.5, inplace=False)\\n    (3): Conv2d(6, 12, kernel_size=(7, 7), stride=(1, 1), padding=(1, 1))\\n    (4): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\\n    (5): ReLU(inplace=True)\\n    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\\n    (7): Conv2d(12, 18, kernel_size=(7, 7), stride=(1, 1), padding=(1, 1))\\n    (8): ReLU(inplace=True)\\n    (9): Dropout(p=0.5, inplace=False)\\n    (10): Conv2d(18, 24, kernel_size=(7, 7), stride=(1, 1), padding=(1, 1))\\n    (11): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\\n    (12): ReLU(inplace=True)\\n    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\\n    (14): Conv2d(24, 30, kernel_size=(7, 7), stride=(1, 1), padding=(1, 1))\\n    (15): ReLU(inplace=True)\\n    (16): Dropout(p=0.5, inplace=False)\\n    (17): Conv2d(30, 36, kernel_size=(7, 7), stride=(1, 1), padding=(1, 1))\\n    (18): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\\n    (19): ReLU(inplace=True)\\n    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\\n    (21): Conv2d(36, 42, kernel_size=(7, 7), stride=(1, 1), padding=(1, 1))\\n    (22): ReLU(inplace=True)\\n    (23): Dropout(p=0.5, inplace=False)\\n    (24): Conv2d(42, 48, kernel_size=(7, 7), stride=(1, 1), padding=(1, 1))\\n    (25): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\\n    (26): ReLU(inplace=True)\\n    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\\n    (28): Conv2d(48, 54, kernel_size=(7, 7), stride=(1, 1), padding=(1, 1))\\n    (29): ReLU(inplace=True)\\n    (30): Dropout(p=0.5, inplace=False)\\n    (31): Conv2d(54, 60, kernel_size=(7, 7), stride=(1, 1), padding=(1, 1))\\n    (32): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\\n    (33): ReLU(inplace=True)\\n    (34): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\\n    (35): Conv2d(60, 66, kernel_size=(7, 7), stride=(1, 1), padding=(1, 1))\\n    (36): ReLU(inplace=True)\\n    (37): Dropout(p=0.5, inplace=False)\\n    (38): Conv2d(66, 72, kernel_size=(7, 7), stride=(1, 1), padding=(1, 1))\\n    (39): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\\n    (40): ReLU(inplace=True)\\n    (41): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\\n    (42): Conv2d(72, 78, kernel_size=(7, 7), stride=(1, 1), padding=(1, 1))\\n    (43): ReLU(inplace=True)\\n    (44): Dropout(p=0.5, inplace=False)\\n    (45): Conv2d(78, 84, kernel_size=(7, 7), stride=(1, 1), padding=(1, 1))\\n    (46): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\\n    (47): ReLU(inplace=True)\\n    (48): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\\n    (49): Conv2d(84, 90, kernel_size=(7, 7), stride=(1, 1), padding=(1, 1))\\n    (50): ReLU(inplace=True)\\n    (51): Dropout(p=0.5, inplace=False)\\n    (52): Conv2d(90, 96, kernel_size=(7, 7), stride=(1, 1), padding=(1, 1))\\n    (53): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\\n    (54): ReLU(inplace=True)\\n    (55): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\\n    (56): Conv2d(96, 102, kernel_size=(7, 7), stride=(1, 1), padding=(1, 1))\\n    (57): ReLU(inplace=True)\\n    (58): Dropout(p=0.5, inplace=False)\\n    (59): Conv2d(102, 108, kernel_size=(7, 7), stride=(1, 1), padding=(1, 1))\\n    (60): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\\n    (61): ReLU(inplace=True)\\n    (62): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\\n    (63): Conv2d(108, 114, kernel_size=(7, 7), stride=(1, 1), padding=(1, 1))\\n    (64): ReLU(inplace=True)\\n    (65): Dropout(p=0.5, inplace=False)\\n  )\\n  (fc): Linear(in_features=16416, out_features=10, bias=True)\\n)\\n\",\n          \"DynamicRNNNet(\\n  (rnn_layers): ModuleList(\\n    (0): RNN(10, 16, batch_first=True)\\n    (1): RNN(16, 32, batch_first=True, bidirectional=True)\\n    (2): RNN(32, 48, batch_first=True)\\n  )\\n  (dropouts): ModuleList(\\n    (0): Dropout(p=0.3, inplace=False)\\n    (1): Dropout(p=0, inplace=False)\\n    (2): Dropout(p=0.3, inplace=False)\\n  )\\n  (fc): Linear(in_features=48, out_features=10, bias=True)\\n)\\n\",\n          \"DynamicRNNNet(\\n  (rnn_layers): ModuleList(\\n    (0): RNN(10, 4, batch_first=True)\\n    (1): RNN(4, 8, batch_first=True, bidirectional=True)\\n    (2): RNN(8, 12, batch_first=True)\\n    (3): RNN(12, 16, batch_first=True, bidirectional=True)\\n    (4): RNN(16, 20, batch_first=True)\\n    (5): RNN(20, 24, batch_first=True, bidirectional=True)\\n    (6): RNN(24, 28, batch_first=True)\\n    (7): RNN(28, 32, batch_first=True, bidirectional=True)\\n    (8): RNN(32, 36, batch_first=True)\\n    (9): RNN(36, 40, batch_first=True, bidirectional=True)\\n    (10): RNN(40, 44, batch_first=True)\\n    (11): RNN(44, 48, batch_first=True, bidirectional=True)\\n    (12): RNN(48, 52, batch_first=True)\\n    (13): RNN(52, 56, batch_first=True, bidirectional=True)\\n    (14): RNN(56, 60, batch_first=True)\\n  )\\n  (dropouts): ModuleList(\\n    (0): Dropout(p=0.1, inplace=False)\\n    (1): Dropout(p=0, inplace=False)\\n    (2): Dropout(p=0.1, inplace=False)\\n    (3): Dropout(p=0, inplace=False)\\n    (4): Dropout(p=0.1, inplace=False)\\n    (5): Dropout(p=0, inplace=False)\\n    (6): Dropout(p=0.1, inplace=False)\\n    (7): Dropout(p=0, inplace=False)\\n    (8): Dropout(p=0.1, inplace=False)\\n    (9): Dropout(p=0, inplace=False)\\n    (10): Dropout(p=0.1, inplace=False)\\n    (11): Dropout(p=0, inplace=False)\\n    (12): Dropout(p=0.1, inplace=False)\\n    (13): Dropout(p=0, inplace=False)\\n    (14): Dropout(p=0.1, inplace=False)\\n  )\\n  (fc): Linear(in_features=60, out_features=10, bias=True)\\n)\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model_descriptions_structured\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 364,\n        \"samples\": [\n          \"Layer1(Type=LSTM | Input_size=10 | Hidden_size=32 | Bidirectional=False)\\nLayer2(Type=LSTM | Input_size=32 | Hidden_size=64 | Bidirectional=True)\\nLayer3(Type=LSTM | Input_size=64 | Hidden_size=96 | Bidirectional=False)\\nLayer4(Type=LSTM | Input_size=96 | Hidden_size=128 | Bidirectional=True)\\nLayer5(Type=Dropout | Probability=0.5 | Inplace=False)\\nLayer6(Type=Dropout | Probability=0.5 | Inplace=False)\\nLayer7(Type=Linear | In_features=256 | Out_features=10 | Bias=True)\",\n          \"Layer1(Type=Conv2d | In_channels=3 | Out_channels=6 | Kernel_size=(5,5) | Stride=(1,1) | Padding=(1,1))\\nLayer2(Type=Conv2d | In_channels=6 | Out_channels=12 | Kernel_size=(5,5) | Stride=(1,1) | Padding=(1,1))\\nLayer3(Type=Conv2d | In_channels=12 | Out_channels=18 | Kernel_size=(5,5) | Stride=(1,1) | Padding=(1,1))\\nLayer4(Type=Conv2d | In_channels=18 | Out_channels=24 | Kernel_size=(5,5) | Stride=(1,1) | Padding=(1,1))\\nLayer5(Type=ReLU | Inplace=True)\\nLayer6(Type=ReLU | Inplace=True)\\nLayer7(Type=ReLU | Inplace=True)\\nLayer8(Type=ReLU | Inplace=True)\\nLayer9(Type=Dropout | Probability=0.5 | Inplace=False)\\nLayer10(Type=Dropout | Probability=0.5 | Inplace=False)\\nLayer11(Type=MaxPool2d | Kernel_size=2 | Stride=2 | Padding=0 | Dilation=1 | Ceil_mode=False)\\nLayer12(Type=MaxPool2d | Kernel_size=2 | Stride=2 | Padding=0 | Dilation=1 | Ceil_mode=False)\\nLayer13(Type=Linear | In_features=600 | Out_features=10 | Bias=True)\",\n          \"Layer1(Type=Conv2d | In_channels=3 | Out_channels=6 | Kernel_size=(3,3) | Stride=(1,1) | Padding=(1,1))\\nLayer2(Type=Conv2d | In_channels=6 | Out_channels=12 | Kernel_size=(3,3) | Stride=(1,1) | Padding=(1,1))\\nLayer3(Type=Conv2d | In_channels=12 | Out_channels=18 | Kernel_size=(3,3) | Stride=(1,1) | Padding=(1,1))\\nLayer4(Type=Conv2d | In_channels=18 | Out_channels=24 | Kernel_size=(3,3) | Stride=(1,1) | Padding=(1,1))\\nLayer5(Type=Conv2d | In_channels=24 | Out_channels=30 | Kernel_size=(3,3) | Stride=(1,1) | Padding=(1,1))\\nLayer6(Type=Conv2d | In_channels=30 | Out_channels=36 | Kernel_size=(3,3) | Stride=(1,1) | Padding=(1,1))\\nLayer7(Type=Conv2d | In_channels=36 | Out_channels=42 | Kernel_size=(3,3) | Stride=(1,1) | Padding=(1,1))\\nLayer8(Type=Conv2d | In_channels=42 | Out_channels=48 | Kernel_size=(3,3) | Stride=(1,1) | Padding=(1,1))\\nLayer9(Type=Conv2d | In_channels=48 | Out_channels=54 | Kernel_size=(3,3) | Stride=(1,1) | Padding=(1,1))\\nLayer10(Type=Conv2d | In_channels=54 | Out_channels=60 | Kernel_size=(3,3) | Stride=(1,1) | Padding=(1,1))\\nLayer11(Type=Conv2d | In_channels=60 | Out_channels=66 | Kernel_size=(3,3) | Stride=(1,1) | Padding=(1,1))\\nLayer12(Type=Conv2d | In_channels=66 | Out_channels=72 | Kernel_size=(3,3) | Stride=(1,1) | Padding=(1,1))\\nLayer13(Type=Conv2d | In_channels=72 | Out_channels=78 | Kernel_size=(3,3) | Stride=(1,1) | Padding=(1,1))\\nLayer14(Type=Conv2d | In_channels=78 | Out_channels=84 | Kernel_size=(3,3) | Stride=(1,1) | Padding=(1,1))\\nLayer15(Type=Conv2d | In_channels=84 | Out_channels=90 | Kernel_size=(3,3) | Stride=(1,1) | Padding=(1,1))\\nLayer16(Type=Conv2d | In_channels=90 | Out_channels=96 | Kernel_size=(3,3) | Stride=(1,1) | Padding=(1,1))\\nLayer17(Type=ReLU | Inplace=True)\\nLayer18(Type=ReLU | Inplace=True)\\nLayer19(Type=ReLU | Inplace=True)\\nLayer20(Type=ReLU | Inplace=True)\\nLayer21(Type=ReLU | Inplace=True)\\nLayer22(Type=ReLU | Inplace=True)\\nLayer23(Type=ReLU | Inplace=True)\\nLayer24(Type=ReLU | Inplace=True)\\nLayer25(Type=ReLU | Inplace=True)\\nLayer26(Type=ReLU | Inplace=True)\\nLayer27(Type=ReLU | Inplace=True)\\nLayer28(Type=ReLU | Inplace=True)\\nLayer29(Type=ReLU | Inplace=True)\\nLayer30(Type=ReLU | Inplace=True)\\nLayer31(Type=ReLU | Inplace=True)\\nLayer32(Type=ReLU | Inplace=True)\\nLayer33(Type=Dropout | Probability=0.5 | Inplace=False)\\nLayer34(Type=Dropout | Probability=0.5 | Inplace=False)\\nLayer35(Type=Dropout | Probability=0.5 | Inplace=False)\\nLayer36(Type=Dropout | Probability=0.5 | Inplace=False)\\nLayer37(Type=Dropout | Probability=0.5 | Inplace=False)\\nLayer38(Type=Dropout | Probability=0.5 | Inplace=False)\\nLayer39(Type=Dropout | Probability=0.5 | Inplace=False)\\nLayer40(Type=Dropout | Probability=0.5 | Inplace=False)\\nLayer41(Type=MaxPool2d | Kernel_size=2 | Stride=2 | Padding=0 | Dilation=1 | Ceil_mode=False)\\nLayer42(Type=MaxPool2d | Kernel_size=2 | Stride=2 | Padding=0 | Dilation=1 | Ceil_mode=False)\\nLayer43(Type=MaxPool2d | Kernel_size=2 | Stride=2 | Padding=0 | Dilation=1 | Ceil_mode=False)\\nLayer44(Type=MaxPool2d | Kernel_size=2 | Stride=2 | Padding=0 | Dilation=1 | Ceil_mode=False)\\nLayer45(Type=MaxPool2d | Kernel_size=2 | Stride=2 | Padding=0 | Dilation=1 | Ceil_mode=False)\\nLayer46(Type=MaxPool2d | Kernel_size=2 | Stride=2 | Padding=0 | Dilation=1 | Ceil_mode=False)\\nLayer47(Type=MaxPool2d | Kernel_size=2 | Stride=2 | Padding=0 | Dilation=1 | Ceil_mode=False)\\nLayer48(Type=MaxPool2d | Kernel_size=2 | Stride=2 | Padding=0 | Dilation=1 | Ceil_mode=False)\\nLayer49(Type=Linear | In_features=0 | Out_features=10 | Bias=True)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model_parameters\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4906395,\n        \"min\": 114,\n        \"max\": 34912010,\n        \"num_unique_values\": 389,\n        \"samples\": [\n          618,\n          1507762,\n          616586\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model_summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1040,\n        \"samples\": [\n          \"The model is a DynamicConvNet, which is a convolutional neural network (CNN) with dynamically changing parameters. The model consists of 59 layers, including 49 convolutional layers, 10 batch normalization layers, 10 max pooling layers, 3 dropout layers, and a fully connected layer. The input to the model is a 3-channel image of arbitrary size. The output of the model is a 10-dimensional vector of probabilities, representing the likelihood of the input image belonging to each of the 10 classes. The model has approximately 3 million trainable parameters.\",\n          \"This model is a Transformer network designed for natural language processing tasks. It consists of an encoder-decoder structure with attention mechanisms. The encoder contains 5 TransformerEncoderLayer instances, each with a self-attention layer and two fully connected layers. The decoder also includes 5 instances of TransformerDecoderLayer, each with self-attention, encoder-decoder attention, and three fully connected layers. The input size of the model is 128 and the output size is 10, representing the probability distribution over 10 different classes. The total number of trainable parameters in this model is 2,811,530.\",\n          \"The DynamicConvNet model consists of 15 convolutional layers and one fully connected layer. The first 14 convolutional layers are arranged in a series of three blocks, each followed by a max pooling layer. Each block contains two convolutional layers, a ReLU activation layer, a dropout layer, and a batch normalization layer (except for the first block). The final convolutional layer is followed by a global average pooling layer. The fully connected layer has an input size of 1470 and an output size of 10. The model has a total of 132970 trainable parameters.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('model_info_2.csv', index=False)  # Optional: exclude index column"
      ],
      "metadata": {
        "id": "9u2YrKYPM7-N"
      },
      "execution_count": 65,
      "outputs": []
    }
  ]
}